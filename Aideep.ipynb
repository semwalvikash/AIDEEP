{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKB10e9NCIolDnaX0VfDdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semwalvikash/AIDEEP/blob/main/Aideep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Install required packages\n",
        "!pip install -q fastapi uvicorn pyngrok transformers accelerate chromadb sentence-transformers pypdf httpx rich\n"
      ],
      "metadata": {
        "id": "lwJphTtE5Wtq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Set up directories\n",
        "!mkdir -p deep-shiva/data/sources"
      ],
      "metadata": {
        "id": "ffMAVt4Q6WLC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ models.py\n",
        "%%writefile deep-shiva/models.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class HFModel:\n",
        "    def __init__(self, model_name=\"microsoft/phi-2\", device=None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32\n",
        "        ).to(self.device)\n",
        "\n",
        "    def generate(self, prompt: str, system: str = \"\", max_new_tokens=300):\n",
        "        full_prompt = (system + \"\\n\\n\" + prompt).strip()\n",
        "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN5B36qu6hJU",
        "outputId": "7a0a536c-5ee2-4529-f944-1bd7f82e0944"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deep-shiva/models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4️⃣ rag.py\n",
        "%%writefile deep-shiva/rag.py\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "\n",
        "class RAG:\n",
        "    def __init__(self):\n",
        "        self.client = chromadb.PersistentClient(path=\"rag_store\")\n",
        "        self.collection = self.client.get_or_create_collection(\"knowledge\")\n",
        "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    def ingest_pdf(self, path):\n",
        "        reader = PdfReader(path)\n",
        "        chunks = []\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                for para in text.split(\"\\n\\n\"):\n",
        "                    if len(para.strip()) > 30:\n",
        "                        chunks.append(para.strip())\n",
        "        if chunks:\n",
        "            embeds = self.embedder.encode(chunks).tolist()\n",
        "            self.collection.add(\n",
        "                documents=chunks,\n",
        "                embeddings=embeds,\n",
        "                metadatas=[{\"source\": path}] * len(chunks),\n",
        "                ids=[f\"{path}_{i}\" for i in range(len(chunks))]\n",
        "            )\n",
        "\n",
        "    def search(self, query, k=4):\n",
        "        emb = self.embedder.encode([query]).tolist()[0]\n",
        "        results = self.collection.query(query_embeddings=[emb], n_results=k)\n",
        "        return list(zip(results[\"documents\"][0], results[\"metadatas\"][0]))\n",
        "\n",
        "    def format_snippets(self, pairs):\n",
        "        return \"\\n\".join([f\"- {doc} (from {meta['source']})\" for doc, meta in pairs])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP-GBJxY6rn3",
        "outputId": "69542ee0-d0f4-4ff2-eea2-8957e7129c1b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deep-shiva/rag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ prompts.py\n",
        "%%writefile deep-shiva/prompts.py\n",
        "SYSTEM_BASE = \"\"\"You are Deep-Shiva, a healthcare info assistant.\n",
        "- You are not a doctor.\n",
        "- Give factual, verified, simple explanations.\n",
        "- If emergency signs appear, tell user to seek help.\"\"\"\n",
        "\n",
        "ANSWER_TEMPLATE = \"\"\"\n",
        "User asked: {question}\n",
        "\n",
        "Relevant knowledge:\n",
        "{snippets}\n",
        "\n",
        "Your helpful answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Tk3jJM65DA",
        "outputId": "53dbccae-144d-46b0-cb01-806417619176"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deep-shiva/prompts.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6️⃣ safety.py\n",
        "%%writefile deep-shiva/safety.py\n",
        "EMERGENCY_KEYWORDS = [\"chest pain\", \"severe bleeding\", \"suicide\", \"heart attack\"]\n",
        "\n",
        "EMERGENCY_RESPONSE = (\n",
        "    \"⚠️ This looks like an emergency. Please call your nearest doctor or emergency helpline immediately.\"\n",
        ")\n",
        "\n",
        "def triage_level(query: str):\n",
        "    q = query.lower()\n",
        "    if any(word in q for word in EMERGENCY_KEYWORDS):\n",
        "        return \"emergency\"\n",
        "    return \"normal\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd21aWHW670V",
        "outputId": "d7481df3-25ca-44d1-a147-aa83a2261415"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deep-shiva/safety.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7️⃣ app.py\n",
        "%%writefile deep-shiva/app.py\n",
        "from fastapi import FastAPI, UploadFile, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "from typing import Optional\n",
        "from rag import RAG\n",
        "from prompts import SYSTEM_BASE, ANSWER_TEMPLATE\n",
        "from safety import triage_level, EMERGENCY_RESPONSE\n",
        "from models import HFModel\n",
        "\n",
        "app = FastAPI(title=\"Deep-Shiva\", version=\"0.1.0\")\n",
        "rag = RAG()\n",
        "llm = HFModel()\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "@app.post(\"/ingest\")\n",
        "async def ingest(file: UploadFile):\n",
        "    path = f\"deep-shiva/data/sources/{file.filename}\"\n",
        "    with open(path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "    rag.ingest_pdf(path)\n",
        "    return {\"ok\": True, \"file\": file.filename}\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(query: str = Form(...), session_id: Optional[str] = Form(None)):\n",
        "    level = triage_level(query)\n",
        "    if level == \"emergency\":\n",
        "        return JSONResponse({\"level\": level, \"answer\": EMERGENCY_RESPONSE})\n",
        "\n",
        "    pairs = rag.search(query, k=4)\n",
        "    snippets = rag.format_snippets(pairs)\n",
        "    prompt = ANSWER_TEMPLATE.format(question=query, snippets=snippets)\n",
        "\n",
        "    answer = llm.generate(prompt, system=SYSTEM_BASE)\n",
        "    disclaimer = \"\\n\\n— This is general health information, not a diagnosis.\"\n",
        "\n",
        "    return {\n",
        "        \"level\": level,\n",
        "        \"answer\": answer + disclaimer,\n",
        "        \"sources\": list({p[1]['source'] for p in pairs}),\n",
        "        \"session\": session_id or \"anonymous\",\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCgGol7_6_2a",
        "outputId": "e5454ae9-04c4-4818-b2d6-bb4e7043f55a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deep-shiva/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken YOUR_AUTHTOKEN_HERE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJgwzsKk7MBT",
        "outputId": "db09a4b9-3dac-464b-febf-10ebe91433f7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/semwalvikash/AIDEEP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ5yWD9yBmJQ",
        "outputId": "1e2c91f1-6d7a-405c-cae1-c733f41116e1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}